"""Network class and main algorithm for training the network."""
from test_model import *
import random

# Add parameters for plotting. 
mpl.rcParams['figure.titleweight'] = "bold"
mpl.rcParams['font.size'] = 14
mpl.rcParams['axes.titlesize'] = 13
mpl.rcParams['axes.titleweight'] = "bold"

class Parameters:
    """Class for the parameters in the Neural Network."""
    def __init__(self,K,d,I):
        self.W_k = np.random.randn(K,d,d)
        self.b_k = np.random.randn(K,d,1)
        self.b_k_I = np.zeros((K,d,I))

        for i in range(K):
            self.b_k_I[i,:,:] = self.b_k[i,:,:]

        self.w = np.random.randn(d)  
        self.my = np.random.rand()
        self.K = K
        
        # For Adam descent.
        self.beta_1 = 0.9; self.beta_2 = 0.999; self.alpha = 0.01; self.epsilon = 10E-8
        self.v = [0]; self.m = [0]
         
    def update_parameters(self,gradient,method,tau,j):
        """Update the parameters via Vanilla Gradient Method or Adam Descent."""
        if(method == "vanilla"):
            self.W_k = self.W_k - tau*gradient[0]
            self.b_k = self.b_k - tau*gradient[1]
            for i in range(self.K):
                self.b_k_I[i,:,:] = self.b_k[i,:,:]
            self.w = self.w - tau*gradient[2]
            self.my = self.my - tau*gradient[3]
            
        elif(method == "adams"):
            g = gradient 
            self.m.append( self.beta_1*self.m[j-1] + (1-self.beta_1)*g)
            self.v.append(self.beta_2*self.v[j-1]+(1-self.beta_2)*(g*g))
            m_hat = self.m[j]/(1-self.beta_1**j)
            v_hat = self.v[j]/(1-self.beta_2**j)
            subtract = self.alpha*m_hat/(self.root(self.v[j])+self.epsilon)
            
            self.W_k = self.W_k - subtract[0]
            self.b_k = self.b_k - subtract[1]
            for i in range(self.K):
                self.b_k_I[i,:,:] = self.b_k[i,:,:]
            self.w = self.w - subtract[2]
            self.my = self.my - subtract[3]      
                      
        else:
            print("No method found") 
    
    def root(self, v_j):
        """Calculate the root of v_j componentwise for Adam descent."""
        for i in range(4):
            v_j[i] = np.sqrt(v_j[i])
        return v_j

class Network:
    """Class for the Neural Network (ResNet)."""
    def __init__(self,K,d,I,h,Z_0,c):
        self.theta = Parameters(K,d,I)
        self.Z_list = np.zeros((K+1,d,I))
        self.Z_list[0,:,:] = Z_0
        self.c = c
        self.h = h
        self.K = K
        self.I = I
        self.d = d
        self.Y = None

        self.J_last = None # For use in the testing (instead of returning J from algorithm, save in NN)
    
    def J(self): 
        """Objective function."""
        return 0.5 * la.norm(self.Y - self.c)**2
    
    def sigma(self, x):
        """Sigmoid activation function."""
        return np.tanh(x)

    def sigma_der(self, x):
        """Derivative of the sigmoid activation function."""
        val = 1-np.tanh(x)**2
        return val

    def eta(self, x):
        """Hypothesis function. Appears in the final layer of the neural network."""
        val = x
        #val = 0.5*(1+ np.tanh(x/2))
        return val

    def eta_der(self, x):
        """The derivative of the hypothesis function."""
        val = np.ones(x.shape)
        #val = 0.25*self.sigma_der(x/2) 
        return val

    def forward_function(self): 
        """Calculate  Y."""
        for i in range(self.K):
            self.Z_list[i+1,:,:] = self.Z_list[i,:,:] + \
                self.h*self.sigma(self.theta.W_k[i,:,:] @ \
                    self.Z_list[i,:,:] + self.theta.b_k_I[i,:,:])
            
        ZT_K = np.transpose(self.Z_list[-1,:,:]) #Enklere notasjon
        one_vec = np.ones(self.I)
        Y = self.eta(ZT_K @ self.theta.w + self.theta.my*one_vec)
        self.Y = Y

    def back_propagation(self):
        """Calculate and return the gradient of the objective function."""
        ZT_K = np.transpose(self.Z_list[-1,:,:])
        one_vec = np.ones(self.I)
        J_der_my =  self.eta_der(ZT_K @ self.theta.w + self.theta.my*one_vec).T @ (self.Y-self.c)
        J_der_w = self.Z_list[-1,:,:] @ ((self.Y-self.c) * \
                            self.eta_der(ZT_K @ self.theta.w + self.theta.my*one_vec))
        
        P_K = np.outer(self.theta.w,(self.Y-self.c)*self.eta_der(ZT_K @ \
                                            self.theta.w + self.theta.my*one_vec)) 
        
        
        P_list = np.zeros((self.K,self.d,self.I)) 
        P_list[-1,:,:] = P_K 
        for i in range(self.K-1,0,-1):
            P_list[i-1,:,:] = P_list[i,:,:] + self.h*np.transpose(self.theta.W_k[i-1,:,:]) @ \
            (self.sigma_der(self.theta.W_k[i-1,:,:] @ self.Z_list[i-1,:,:] + \
                            self.theta.b_k_I[i-1,:,:]) * P_list[i,:,:])

        J_der_Wk = np.zeros((self.K,self.d,self.d))
        J_der_b = np.zeros((self.K,self.d,1))
        one_vec = np.ones((self.I,1)) 
        
        for i in range(self.K):
            val = P_list[i,:,:] * self.sigma_der(self.theta.W_k[i,:,:] @ \
                                                 self.Z_list[i,:,:] + self.theta.b_k_I[i,:,:])
            J_der_Wk[i,:,:] = self.h*(val @ np.transpose(self.Z_list[i,:,:]))
            J_der_b[i,:,:] = self.h*(val @ one_vec)
        
        gradient = np.array((J_der_Wk,J_der_b,J_der_w,J_der_my))
        
        return gradient
    
    def embed_input_and_sol(self, test_input, test_sol):
        """Embed the input into d-dimensional space."""
        if len(test_input.shape) == 1:
            I_new = 1
        else:
            I_new = test_input.shape[1]
        self.I = I_new

        self.Z_list = np.zeros((self.K+1,self.d,self.I))
        self.Z_list[0,:,:] = test_input
        self.c = test_sol

        self.theta.b_k_I = np.zeros((self.K,self.d,self.I))
        for i in range(self.K):
            self.theta.b_k_I[i,:,:] = self.theta.b_k[i,:,:]
    
    def calculate_output(self, input):
        """Calculates and returns the networks output from a given input"""
        self.embed_test_input(input,input)
        self.forward_function()
        print("Ys shape: \n")
        print(self.Y.shape)
        return self.Y



    def Hamiltonian_gradient(self):
        """Calculate the gradient of F, according to the theoretical derivation.
        
        In general it is used for the entire Hamiltonian F, but since we are 
        working with separable Hamiltonians, it can be used on both T and V.
        """
        one_vec = np.ones((self.I,1)) 

    
        w_grad = self.theta.w.reshape((self.d,1)) #need different dimensions for w 
        gradient = w_grad @ self.eta_der(w_grad.T@self.Z_list[self.K,:,:] + self.theta.my*one_vec.T) 
                                                    

        for k in range(self.K - 1, -1, -1):
            gradient += self.theta.W_k[k,:,:].T @ (self.h*self.sigma_der(\
                    self.theta.W_k[k,:,:]@self.Z_list[k,:,:] + self.theta.b_k_I[k,:,:])*gradient)
        
        return gradient

def algorithm(I, d, d0, K, h, iterations, tau, chunk, function, domain, scaling, alpha, beta, plot = False, savename = ""):
    """Main training algorithm."""
      
    inp = generate_input(function,domain,d0,I,d)
    output = get_solution(function,inp,d,I,d0)

    if scaling:
        inp, a1, b1 = scale_data(alpha,beta,inp)
        output, a2, b2 = scale_data(alpha,beta,output)


    Z_0 = inp[:,0:chunk]
    c_0 = output[0:chunk]
    NN = Network(K,d,chunk,h,Z_0,c_0)
    
    # For plotting J. 
    J_list = np.zeros(iterations)
    it = np.zeros(iterations)

    counter = 0
    for j in range(1,iterations+1):
        
        NN.forward_function()
        gradient = NN.back_propagation()
        NN.theta.update_parameters(gradient,"adams",tau,j)

        if counter < I/chunk - 1:
            NN.Z_list[0,:,:] = inp[:,chunk*(counter+1):chunk*(counter+2)]
            NN.c = output[chunk*(counter + 1):chunk*(counter + 2)]
        else:
            counter = 0

        J_list[j-1] = NN.J()
        it[j-1] = j
    
    if plot:
        fig, ax = plt.subplots()
        ax.plot(it,J_list)
        fig.suptitle("Objective Function J as a Function of Iterations.", fontweight = "bold")
        ax.set_ylabel("J")
        ax.set_xlabel("Iteration")
        plt.text(0.5, 0.5, "Value of J at iteration "+str(iterations)+": "+str(round(J_list[-1], 4)), 
                horizontalalignment="center", verticalalignment="center", 
                transform=ax.transAxes, fontsize = 16)
        if savename != "": 
            plt.savefig(savename, bbox_inches='tight')
        plt.show()
    
    
    NN.J_last = J_list[-1] # Save last value of J_list in NN, to check which converges best in tests. 
    
    return NN

def algorithm_sgd(I,d, d0, K,h,iterations, tau, chunk, function,domain,scaling, alpha, beta, plot = False, savename = ""):
    """Main training algorithm."""

    input = generate_input(function,domain,d0,I,d)
    output = get_solution(function,input,d,I,d0)

    if scaling:
        input, a1, b1 = scale_data(alpha,beta,input)
        output, a2, b2 = scale_data(alpha,beta,output)

    index_list = [i for i in range(I)]

    Z_0, c_0, index_list = get_random_sample(input,output,index_list,chunk,d)
    NN = Network(K,d,chunk,h,Z_0,c_0)

    # For plotting J. 
    J_list = np.zeros(iterations)
    it = np.zeros(iterations)

    counter = 0
    for j in range(1,iterations+1):


        NN.forward_function()
        gradient = NN.back_propagation()
        NN.theta.update_parameters(gradient,"adams",tau,j)

        #For plotting
        J_list[j-1] = NN.J()
        it[j-1] = j

        if counter < I/chunk - 1:
            Z, c, index_list = get_random_sample(input,output,index_list,chunk,d)
            NN.Z_list[0,:,:] = Z
            NN.c = c
            counter += 1
        else:
            #All data has been sifted through
            counter = 0
            index_list = [i for i in range(I)]


    if plot:
        fig, ax = plt.subplots()
        ax.plot(it,J_list)
        fig.suptitle("Objective Function J as a Function of Iterations.", fontweight = "bold")
        ax.set_ylabel("J")
        ax.set_xlabel("Iteration")
        plt.text(0.5, 0.5, "Value of J at iteration "+str(iterations)+": "+str(round(J_list[-1], 4)), 
                horizontalalignment="center", verticalalignment="center", 
                transform=ax.transAxes, fontsize = 16)
        if savename != "": 
            plt.savefig(savename, bbox_inches='tight')
        plt.show()
    return NN 

def get_random_sample(input, sol, index_list, chunk, d):
    sample = np.zeros((d,chunk))
    sample_sol = np.zeros(chunk)
    random_indices = random.sample(index_list,chunk)

    for i in range(chunk):
        rand_index = random_indices[i]
        index_list.remove(rand_index)
        sample[:,i] = input[:,rand_index]
        sample_sol[i] = sol[rand_index]

    return sample, sample_sol, index_list
